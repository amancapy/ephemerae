{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from itertools import combinations\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import tensorflow as tf\n",
    "from keras import models, layers, losses, optimizers, regularizers, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 25)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain_index = pickle.load(open(\"../data/support.pkl\", \"rb\"))\n",
    "nouns, verbs = pickle.load(open(\"../data/vecs.pkl\", \"rb\"))\n",
    "\n",
    "pickles = [pickle.load(open(f\"../data/pickles/{i}.pkl\", \"rb\")) for i in range(1)]\n",
    "pickles = [item for sublist in pickles for item in sublist]\n",
    "pickles = sorted(pickles, key=lambda x: x[1])\n",
    "pickles = [[item for item in pickles if item[1] == noun] for noun in nouns]\n",
    "pickles = [(np.add.reduce([item[0] for item in sublist]) / len(sublist), sublist[0][1]) for sublist in pickles]\n",
    "pickles = [(item[0][brain_index], item[1]) for item in pickles]\n",
    "\n",
    "len(nouns), len(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2(a, b):\n",
    "    return norm(np.subtract(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasisSum(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.basis = tf.Variable(tf.convert_to_tensor([verbs[verb] for verb in verbs]), trainable=False, name=\"verb_basis\")\n",
    "        self.d1 = layers.Dense(64, activation=\"relu\")\n",
    "        self.d2 = layers.Dense(32, activation=\"relu\")\n",
    "        self.dn = layers.Dense(self.basis.shape[0], activation=\"sigmoid\")\n",
    "        \n",
    "    @tf.function(reduce_retracing=True)\n",
    "    def call(self, x):\n",
    "        x = self.d1(x)\n",
    "        x = self.d2(x)\n",
    "        x = self.dn(x)\n",
    "        x = x / tf.reduce_sum(x, axis=-1, keepdims=True)\n",
    "        \n",
    "        x = tf.einsum(\"bi,ij->bj\", x, self.basis)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7fa714251c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7fa714251c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "0.003373884856700897\n",
      "0.26873515754938126\n",
      "0.25013391152024267\n",
      "0.24338313892483712\n",
      "0.2459697562456131\n",
      "0.2433212825655937\n",
      "0.24274668991565704\n",
      "0.23745998457074166\n",
      "0.23517379745841027\n",
      "0.23326953530311584\n",
      "0.23542273700237273\n",
      "0.2250434625148773\n",
      "0.21668407633900644\n",
      "0.21342742666602135\n",
      "0.21205770686268807\n",
      "0.2107598027586937\n",
      "0.20970246940851212\n",
      "0.20806237146258355\n",
      "0.2072278454899788\n",
      "0.20947147101163865\n",
      "0.20867252543568612\n",
      "0.20649966582655907\n",
      "0.20681284323334695\n",
      "0.2087359519302845\n",
      "0.2075600107014179\n",
      "0.20598743930459024\n",
      "0.20635671481490137\n",
      "0.20699978485703469\n",
      "0.20754908084869383\n",
      "0.2078784492611885\n",
      "0.2082293750345707\n",
      "0.20688654124736786\n",
      "0.2075345528125763\n",
      "0.20652731388807297\n",
      "0.20615026965737343\n",
      "0.20657311424612998\n",
      "0.20503491029143334\n",
      "0.2047408950328827\n",
      "0.20672660782933236\n",
      "0.20743891596794128\n",
      "0.20780995905399322\n",
      "0.20698378771543502\n",
      "0.20737035363912582\n",
      "0.20749947026371957\n",
      "0.20702317222952843\n",
      "0.20729136556386948\n",
      "0.20632491052150725\n",
      "0.2079425023496151\n",
      "0.20572543159127235\n",
      "0.2084750524163246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 1.000: : 1it [01:09, 69.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0032445475459098815\n",
      "0.2780319565534592\n",
      "0.2712273408472538\n",
      "0.24383723929524423\n",
      "0.21496526032686233\n",
      "0.21384094297885894\n",
      "0.2134184142947197\n",
      "0.21003156304359435\n",
      "0.2119832782447338\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "pbar = tqdm(combinations(range(60), 58))\n",
    "correct_count = 0\n",
    "\n",
    "x = np.array([item[0] for item in pickles])\n",
    "y = [item[1] for item in pickles]\n",
    "y = np.array([nouns[item] for item in y])\n",
    "\n",
    "x, y = tf.cast(x, tf.dtypes.float32), tf.cast(y, tf.dtypes.float32)\n",
    "\n",
    "for i, comb in enumerate(pbar):\n",
    "    comp = list(set.difference(set(range(60)), set(comb)))\n",
    "\n",
    "    model = BasisSum()\n",
    "    loss = losses.MeanSquaredError()\n",
    "    opt = optimizers.Adam(0.001)\n",
    "    \n",
    "    train_x, test_x = tf.gather(x, comb), tf.gather(x, comp)\n",
    "    train_y, test_y = tf.gather(y, comb), tf.gather(y, comp)\n",
    "\n",
    "    batchlosses = []\n",
    "    for j in range(5000):\n",
    "        idx1 = tf.random.uniform(shape=[batch_size], minval=0, maxval=tf.shape(train_x)[0], dtype=tf.int32)\n",
    "        idx2 = tf.random.uniform(shape=[batch_size], minval=0, maxval=tf.shape(train_x)[0], dtype=tf.int32)\n",
    "\n",
    "        batch_x1, batch_y1 = tf.gather(train_x, idx1), tf.gather(train_y, idx1)\n",
    "        batch_x2, batch_y2 = tf.gather(train_x, idx2), tf.gather(train_y, idx2)\n",
    "\n",
    "        ratios = tf.random.uniform((len(batch_x1), 1), 0, 1)\n",
    "        batch_x = batch_x1 * ratios + batch_x2 * (1 - ratios)\n",
    "        batch_y = batch_y1 * ratios + batch_y2 * (1 - ratios)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred_y = model(batch_x)\n",
    "            batchloss = loss(batch_y, pred_y)\n",
    "            grads = tape.gradient(batchloss, model.trainable_variables)\n",
    "            opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "            batchlosses.append(float(batchloss))\n",
    "\n",
    "        if j % 100 == 0:\n",
    "            # print(sum(batchlosses[-100:]) / 100)\n",
    "            ...\n",
    "\n",
    "    pred = model(test_x)\n",
    "    t1, t2 = test_y.numpy()\n",
    "    t1, t2 = t1.flat, t2.flat\n",
    "    p1, p2 = pred.numpy()\n",
    "    p1, p2 = p1.flat, p2.flat\n",
    "    \n",
    "    correct = l2(t1, p1) + l2(t2, p2)\n",
    "    incorrect = l2(t1, p2) + l2(t2, p1)\n",
    "\n",
    "    correct_count += int(correct < incorrect)\n",
    "\n",
    "    pbar.set_description(f\"accuracy: {correct_count / (i + 1):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ephemerai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
