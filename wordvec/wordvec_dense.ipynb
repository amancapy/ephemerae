{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-26 22:48:33.107879: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-26 22:48:33.107912: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-26 22:48:33.107943: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from itertools import combinations\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import tensorflow as tf\n",
    "from keras import models, layers, losses, optimizers, regularizers, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading in brain scans and arranging them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 25)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain_index = pickle.load(open(\"../data/support.pkl\", \"rb\"))\n",
    "nouns, verbs = pickle.load(open(\"../data/vecs.pkl\", \"rb\"))\n",
    "\n",
    "pickles = [pickle.load(open(f\"../data/pickles/{i}.pkl\", \"rb\")) for i in range(1)]\n",
    "pickles = [item for sublist in pickles for item in sublist]\n",
    "pickles = sorted(pickles, key=lambda x: x[1])\n",
    "pickles = [[item for item in pickles if item[1] == noun] for noun in nouns]\n",
    "pickles = [(np.add.reduce([item[0] for item in sublist]) / len(sublist), sublist[0][1]) for sublist in pickles]\n",
    "pickles = [(item[0][brain_index], item[1]) for item in pickles]\n",
    "\n",
    "len(nouns), len(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2(a, b):\n",
    "    return norm(np.subtract(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semi-linear basis-sum model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasisSum(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.basis = tf.Variable(tf.convert_to_tensor([verbs[verb] for verb in verbs]), trainable=False, name=\"verb_basis\")\n",
    "        self.d1 = layers.Dense(64, activation=\"relu\")\n",
    "        self.d2 = layers.Dense(32, activation=\"relu\")\n",
    "        self.dn = layers.Dense(self.basis.shape[0], activation=\"sigmoid\")\n",
    "        \n",
    "    @tf.function(reduce_retracing=True)\n",
    "    def call(self, x):\n",
    "        x = self.d1(x)\n",
    "        x = self.d2(x)\n",
    "        x = self.dn(x)\n",
    "        x = x / tf.reduce_sum(x, axis=-1, keepdims=True)\n",
    "        \n",
    "        x = tf.einsum(\"bi,ij->bj\", x, self.basis)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1770 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7ff4b028fca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7ff4b028fca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "0 0.0032242628931999206\n",
      "100 0.2748565323650837\n",
      "200 0.23284569680690764\n",
      "300 0.2140559086203575\n",
      "400 0.2097366474568844\n",
      "500 0.21185038074851037\n",
      "600 0.21143039390444757\n",
      "700 0.21109800517559052\n",
      "800 0.20855623215436936\n",
      "900 0.2101852323114872\n",
      "1000 0.20782779216766356\n",
      "1100 0.20925382360816003\n",
      "1200 0.2093658658862114\n",
      "1300 0.21002602502703666\n",
      "1400 0.20877980202436447\n",
      "1500 0.20747047111392022\n",
      "1600 0.2105011013150215\n",
      "1700 0.21010493591427803\n",
      "1800 0.20805832713842393\n",
      "1900 0.20935674428939818\n",
      "2000 0.20683040156960486\n",
      "2100 0.20868419080972672\n",
      "2200 0.2089761109650135\n",
      "2300 0.20921524330973626\n",
      "2400 0.2068786506354809\n",
      "2500 0.20866013795137406\n",
      "2600 0.20840485841035844\n",
      "2700 0.20907114908099175\n",
      "2800 0.2076926076412201\n",
      "2900 0.20738729506731032\n",
      "3000 0.20762544840574265\n",
      "3100 0.20777850732207298\n",
      "3200 0.20706646859645844\n",
      "3300 0.2064448857307434\n",
      "3400 0.20801396191120147\n",
      "3500 0.20728522524237633\n",
      "3600 0.20816572085022927\n",
      "3700 0.2063796941936016\n",
      "3800 0.20517071649432184\n",
      "3900 0.20615896299481393\n",
      "4000 0.20834925532341003\n",
      "4100 0.20765911132097245\n",
      "4200 0.20752407491207123\n",
      "4300 0.20572210147976874\n",
      "4400 0.20785005673766135\n",
      "4500 0.2091652475297451\n",
      "4600 0.20800084710121156\n",
      "4700 0.20850635081529617\n",
      "4800 0.2081084954738617\n",
      "4900 0.2064256900548935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 1.000:   0%|          | 1/1770 [01:12<35:26:52, 72.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.003512914180755615\n",
      "100 0.27605067640542985\n",
      "200 0.25945144027471545\n",
      "300 0.24551455706357955\n",
      "400 0.24256567940115928\n",
      "500 0.24200015395879745\n",
      "600 0.24160409614443779\n",
      "700 0.24073706939816475\n",
      "800 0.23940415009856225\n",
      "900 0.23877643585205077\n",
      "1000 0.23962537541985512\n",
      "1100 0.23856934502720833\n",
      "1200 0.23948938965797426\n",
      "1300 0.2413523504137993\n",
      "1400 0.23970524609088897\n",
      "1500 0.23769414514303208\n",
      "1600 0.23856200009584427\n",
      "1700 0.2396504619717598\n",
      "1800 0.23923427656292914\n",
      "1900 0.23618395507335663\n",
      "2000 0.23837308809161187\n",
      "2100 0.2371591244637966\n",
      "2200 0.23616427898406983\n",
      "2300 0.2369704918563366\n",
      "2400 0.23723785683512688\n",
      "2500 0.23923857048153876\n",
      "2600 0.23800556033849715\n",
      "2700 0.23895808562636375\n",
      "2800 0.23941644743084908\n",
      "2900 0.23927636429667473\n",
      "3000 0.2377398493885994\n",
      "3100 0.2406399980187416\n",
      "3200 0.2397163274884224\n",
      "3300 0.23684555113315583\n",
      "3400 0.24103962868452072\n",
      "3500 0.22944209724664688\n",
      "3600 0.22547735765576363\n",
      "3700 0.22130061492323874\n",
      "3800 0.22131802409887313\n",
      "3900 0.2236194059252739\n",
      "4000 0.22105723842978478\n",
      "4100 0.22344194650650023\n",
      "4200 0.220229130089283\n",
      "4300 0.22089523330330849\n",
      "4400 0.22188905894756317\n",
      "4500 0.21980852842330934\n",
      "4600 0.22389577478170394\n",
      "4700 0.22180819988250733\n",
      "4800 0.22157955139875413\n",
      "4900 0.2218477949500084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.500:   0%|          | 2/1770 [02:25<35:48:51, 72.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.003203906416893005\n",
      "100 0.26491287365555766\n",
      "200 0.24773427352309227\n",
      "300 0.24810016185045242\n",
      "400 0.24568409442901612\n",
      "500 0.24598790258169173\n",
      "600 0.24589310616254806\n",
      "700 0.24702388510107995\n",
      "800 0.24405692994594574\n",
      "900 0.24600760832428933\n",
      "1000 0.24683998689055442\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "pbar = tqdm(combinations(range(60), 58), total=1770)\n",
    "\n",
    "x = np.array([item[0] for item in pickles])\n",
    "y = [item[1] for item in pickles]\n",
    "y = np.array([nouns[item] for item in y])\n",
    "\n",
    "x, y = tf.cast(x, tf.dtypes.float32), tf.cast(y, tf.dtypes.float32)\n",
    "correct_count = 0\n",
    "\n",
    "for i, comb in enumerate(pbar):\n",
    "    comp = [j for j in range(60) if j not in comb]\n",
    "\n",
    "    model = BasisSum()\n",
    "    loss = losses.MeanSquaredError()\n",
    "    opt = optimizers.Adam(0.001)\n",
    "\n",
    "    train_x, test_x = tf.gather(x, comb), tf.gather(x, comp)\n",
    "    train_y, test_y = tf.gather(y, comb), tf.gather(y, comp)\n",
    "\n",
    "    batchlosses = []\n",
    "    for j in range(5000):\n",
    "        idx1 = tf.random.uniform(shape=[batch_size], minval=0, maxval=tf.shape(train_x)[0], dtype=tf.int32)\n",
    "        idx2 = tf.random.uniform(shape=[batch_size], minval=0, maxval=tf.shape(train_x)[0], dtype=tf.int32)\n",
    "\n",
    "        batch_x1, batch_y1 = tf.gather(train_x, idx1), tf.gather(train_y, idx1)\n",
    "        batch_x2, batch_y2 = tf.gather(train_x, idx2), tf.gather(train_y, idx2)\n",
    "\n",
    "        ratios = tf.random.uniform((len(batch_x1), 1), 0, 1)\n",
    "        batch_x = batch_x1 * ratios + batch_x2 * (1 - ratios)\n",
    "        batch_y = batch_y1 * ratios + batch_y2 * (1 - ratios)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred_y = model(batch_x)\n",
    "            batchloss = loss(batch_y, pred_y)\n",
    "            grads = tape.gradient(batchloss, model.trainable_variables)\n",
    "            opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "            batchlosses.append(float(batchloss))\n",
    "\n",
    "        if j % 100 == 0:\n",
    "            print(j, sum(batchlosses[-100:]) / 100)\n",
    "            ...\n",
    "            \n",
    "    pred = model(test_x)\n",
    "    t1, t2 = test_y.numpy()\n",
    "    t1, t2 = t1.flat, t2.flat\n",
    "    p1, p2 = pred.numpy()\n",
    "    p1, p2 = p1.flat, p2.flat\n",
    "    \n",
    "    correct = l2(t1, p1) + l2(t2, p2)\n",
    "    incorrect = l2(t1, p2) + l2(t2, p1)\n",
    "\n",
    "    correct_count += int(correct < incorrect)\n",
    "\n",
    "    pbar.set_description(f\"accuracy: {correct_count / (i + 1):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ephemerai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
